# -*- coding: utf-8 -*-
"""Lead Scoring.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BjQ6AHdMgSd-9dX6MX-AwgjjGC9frx06

# Submitted By : Raj Pratap Pandey, Sushma Mahagaonkar, Kaveri Namdeorao Deotkar

# Lead Scoring

## Import Dependencies
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import seaborn as sns

"""## Directory Path"""

data = pd.read_csv('Leads.csv')

"""## Loading and visualizing the data"""

data.head()

data.describe()

data.info()

data.shape

##Checking for duplicates

data.duplicated(subset = ['Prospect ID'], keep = False).sum()
data.duplicated(subset = ['Lead Number'], keep = False).sum()

##There are no duplicate entries in Prospect ID or Lead Number.

##Both the Potential ID and Lead Number are just placeholders for the Contacted People's ID numbers and can be removed.

"""## EDA : Exploratory data analysis"""

##Data Cleaning

#removing Lead Number and Prospect ID because they both have distinct values
data.drop(['Prospect ID', 'Lead Number'], 1, inplace = True)

#transforming "Select" value inputs to NaN.
data = data.replace('Select', np.nan)

data.nunique()

# Now we have to remove unique valued columns
data= data.drop(['Magazine','Receive More Updates About Our Courses','I agree to pay the amount through cheque','Get updates on DM Content','Update me on Supply Chain Content'],axis=1)

#Next look for null values in every row
data.isnull().sum()

round(100*(data.isnull().sum())/len(data.index),2) #look for null value percentage

#remove the columns with null values percentage > 45

data = data.drop(['Asymmetrique Profile Score','Asymmetrique Activity Score','Asymmetrique Profile Index','Asymmetrique Activity Index','Lead Profile','Lead Quality','How did you hear about X Education',],axis =1)
data.shape

#Again we check for null values percentage

round(100*(data.isnull().sum()/len(data.index)), 2)

"""As can be seen above, there are a significant number of null variables in some sections. However, since these are crucial categories, removing the rows with null values will cost us a lot of data. Therefore, we are going to substitute "not supplied" for the NaN values. In this manner, we have almost no null entries and all the data. If these are included in the model, it will be useless and we can discard it at that point."""

data['Specialization'] = data['Specialization'].fillna('not provided')
data['City'] = data['City'].fillna('not provided')
data['Tags'] = data['Tags'].fillna('not provided')
data['What matters most to you in choosing a course'] = data['What matters most to you in choosing a course'].fillna('not provided')
data['What is your current occupation'] = data['What is your current occupation'].fillna('not provided')
data['Country'] = data['Country'].fillna('not provided')
data.info()

#checking null values percentage
round(100*(data.isnull().sum()/len(data.index)), 2)

data.shape

"""## Categorical Attributes Analysis"""

data['Country'].value_counts()

def slots(x):
    category = ""
    if x == "India":
        category = "India"
    elif x == "not provided":
        category = "not provided"
    else:
        category = "outside india"
    return category

data['Country'] = data.apply(lambda x:slots(x['Country']), axis = 1)
data['Country'].value_counts()

#India is the most frequent occurrence among the non-missing values, so we can assume that it represents all absent values.

data['Country'] = data['Country'].replace('not provided','India')
data['Country'].value_counts()

#calculating the percentage of loss after removing the null numbers
round(100*(sum(data.isnull().sum(axis=1) > 1)/data.shape[0]),2)

data = data[data.isnull().sum(axis=1) <1]
round(100*(data.isnull().sum()/len(data.index)), 2)

data.shape

#Plotting the Country column's distribution after removing the NaN values

plt.figure(figsize=(15,5))
s1=sns.countplot(data.Country, hue=data.Converted)
s1.set_xticklabels(s1.get_xticklabels(),rotation=90)
plt.show()

"""Given that India has a significant number of values (nearly 97% of the data), this field can be removed."""

colDrop=['Country'] #make a list of columns for dropping them

data['City'].value_counts(dropna=False) #checking value counts of "City" column

#plot for City columnn
plt.figure(figsize=(10,5))
s1=sns.countplot(data.City, hue=data.Converted)
s1.set_xticklabels(s1.get_xticklabels(),rotation=90)
plt.show()

plt.figure(figsize = (20,40))

plt.subplot(6,2,1)
sns.countplot(data['Lead Origin'])
plt.title('Lead Origin')

plt.subplot(6,2,2)
sns.countplot(data['Do Not Email'])
plt.title('Do Not Email')

plt.subplot(6,2,3)
sns.countplot(data['Do Not Call'])
plt.title('Do Not Call')

plt.subplot(6,2,4)
sns.countplot(data['Country'])
plt.title('Country')

plt.subplot(6,2,5)
sns.countplot(data['Search'])
plt.title('Search')
plt.subplot(6,2,6)
sns.countplot(data['Newspaper Article'])
plt.title('Newspaper Article')

plt.subplot(6,2,7)
sns.countplot(data['X Education Forums'])
plt.title('X Education Forums')

plt.subplot(6,2,8)
sns.countplot(data['Newspaper'])
plt.title('Newspaper')

plt.subplot(6,2,9)
sns.countplot(data['Digital Advertisement'])
plt.title('Digital Advertisement')

plt.subplot(6,2,10)
sns.countplot(data['Through Recommendations'])
plt.title('Through Recommendations')

plt.subplot(6,2,11)
sns.countplot(data['A free copy of Mastering The Interview'])
plt.title('A free copy of Mastering The Interview')
plt.subplot(6,2,12)
sns.countplot(data['Last Notable Activity']).tick_params(axis='x', rotation = 90)
plt.title('Last Notable Activity')


plt.show()

sns.countplot(data['Lead Source']).tick_params(axis='x', rotation = 90)
plt.title('Lead Source')
plt.show()

plt.figure(figsize = (20,30))
plt.subplot(2,2,1)
sns.countplot(data['Specialization']).tick_params(axis='x', rotation = 90)
plt.title('Specialization')
plt.subplot(2,2,2)
sns.countplot(data['What is your current occupation']).tick_params(axis='x', rotation = 90)
plt.title('Current Occupation')
plt.subplot(2,2,3)
sns.countplot(data['What matters most to you in choosing a course']).tick_params(axis='x', rotation = 90)
plt.title('What matters most to you in choosing a course')
plt.subplot(2,2,4)
sns.countplot(data['Last Activity']).tick_params(axis='x', rotation = 90)
plt.title('Last Activity')
plt.show()

sns.countplot(data['Converted'])
plt.title('Converted("Y variable")')
plt.show()

"""## Numerical Variables"""

plt.figure(figsize = (10,10))
plt.subplot(221)
plt.hist(data['TotalVisits'], bins = 200)
plt.title('Total Visits')
plt.xlim(0,25)

plt.subplot(222)
plt.hist(data['Total Time Spent on Website'], bins = 10)
plt.title('Total Time Spent on Website')

plt.subplot(223)
plt.hist(data['Page Views Per Visit'], bins = 20)
plt.title('Page Views Per Visit')
plt.xlim(0,20)
plt.show( )

#Combinig categorical variables
plt.figure(figsize = (10,10))

plt.subplot(2,2,1)
sns.countplot(x='Lead Origin', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Lead Origin')

plt.subplot(2,2,2)
sns.countplot(x='Lead Source', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Lead Source')
plt.show()

plt.figure(figsize=(10 ,5))
plt.subplot(1,2,1)
sns.countplot(x='Do Not Email', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Do Not Email')

plt.subplot(1,2,2)
sns.countplot(x='Do Not Call', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Do Not Call')
plt.show()

plt.figure(figsize = (10,5))

plt.subplot(1,2,1)
sns.countplot(x='Last Activity', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Last Activity')

plt.subplot(1,2,2)
sns.countplot(x='Country', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Country')
plt.show()

plt.figure(figsize = (10,5))

plt.subplot(1,2,1)
sns.countplot(x='Specialization', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Specialization')

plt.subplot(1,2,2)
sns.countplot(x='What is your current occupation', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('What is your current occupation')
plt.show()

plt.figure(figsize = (10,5))

plt.subplot(1,2,1)
sns.countplot(x='What matters most to you in choosing a course', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('What matters most to you in choosing a course')

plt.subplot(1,2,2)
sns.countplot(x='Search', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Search')
plt.show()

plt.figure(figsize = (10,5))

plt.subplot(1,2,1)
sns.countplot(x='Newspaper Article', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Newspaper Article')

plt.subplot(1,2,2)
sns.countplot(x='X Education Forums', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('X Education Forums')
plt.show()

plt.figure(figsize = (10,5))

plt.subplot(1,2,1)
sns.countplot(x='Newspaper', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Newspaper')

plt.subplot(1,2,2)
sns.countplot(x='Digital Advertisement', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Digital Advertisement')
plt.show()

plt.figure(figsize = (10,5))

plt.subplot(1,2,1)
sns.countplot(x='Through Recommendations', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Through Recommendations')

plt.subplot(1,2,2)
sns.countplot(x='A free copy of Mastering The Interview', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('A free copy of Mastering The Interview')
plt.show()

sns.countplot(x='Last Notable Activity', hue='Converted', data= data).tick_params(axis='x', rotation = 90)
plt.title('Last Notable Activity')
plt.show()

#Plotting HeatMap

plt.figure(figsize=(10,5))
sns.heatmap(data.corr())
plt.show()

"""It is clear from the above EDA that many of the components have scant information and will therefore be less relevant to our analysis.

## Outlier
"""

numeric = data[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']]
numeric.describe(percentiles=[0.25,0.5,0.75,0.9,0.99])

plt.figure(figsize = (5,5))
sns.boxplot(y=data['TotalVisits'])
plt.show()

sns.boxplot(y=data['Total Time Spent on Website'])
plt.show()

sns.boxplot(y=data['Page Views Per Visit'])
plt.show()

#presence of outliers in TotalVisits is visible

#Remove top & bottom 1% of the Column Outlier values
Q3 = data.TotalVisits.quantile(0.99)
data = data[(data.TotalVisits <= Q3)]
Q1 = data.TotalVisits.quantile(0.01)
data = data[(data.TotalVisits >= Q1)]
sns.boxplot(y=data['TotalVisits'])
plt.show()

#Dummy Variables

#list of columns to be dropped
colDrop=['Country','Tags']

"""We can remove "Tags" because they are produced by the sales team after discussion with the students, but keeping them would improve the model's accuracy."""

data = data.drop(colDrop,1)
data.info()

#list of categorical columns
catCol= data.select_dtypes(include=['object']).columns
catCol

# Create dummy variables using the 'get_dummies'
dummy = pd.get_dummies(data[['Lead Origin','Specialization' ,'Lead Source', 'Do Not Email', 'Last Activity', 'What is your current occupation','A free copy of Mastering The Interview', 'Last Notable Activity']], drop_first=True)

LD_dummy = pd.concat([data, dummy], axis=1)
LD_dummy

LD_dummy = LD_dummy.drop(['City','What is your current occupation_not provided','Lead Origin', 'Lead Source', 'Do Not Email', 'Do Not Call','Last Activity', 'Specialization', 'Specialization_not provided','What is your current occupation','What matters most to you in choosing a course', 'Search','Newspaper Article', 'X Education Forums', 'Newspaper','Digital Advertisement', 'Through Recommendations','A free copy of Mastering The Interview', 'Last Notable Activity'], 1)
LD_dummy

"""## Split Training and Testing data"""

#Import the required library
from sklearn.model_selection import train_test_split

LSx = LD_dummy.drop(['Converted'], 1)
LSx.head()

LSy = LD_dummy['Converted']
LSy.head()

#70% and 30% for train and test respectively
LSx_train, LSx_test, LSy_train, LSy_test = train_test_split(LSx, LSy, train_size=0.7, test_size=0.3, random_state=10)

from sklearn.preprocessing import MinMaxScaler

# Scale the three numeric features
scaler = MinMaxScaler()
LSx_train[['TotalVisits', 'Page Views Per Visit', 'Total Time Spent on Website']] = scaler.fit_transform(LSx_train[['TotalVisits', 'Page Views Per Visit', 'Total Time Spent on Website']])
LSx_train.head()

"""## Preparing the Model"""

import statsmodels.api as sm

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()

from sklearn.feature_selection import RFE
rfe = RFE(logreg,n_features_to_select=15)            # running RFE with 15 variables as output
rfe = rfe.fit(LSx_train, LSy_train)

col = LSx_train.columns[rfe.support_] #Place all of the columns RFE selected in the variable "col"

"""Put all of the sections chosen by RFE into the variable 'col'."""

LSx_train = LSx_train[col] # Selecting columns selected by RFE

LSx_train_sm = sm.add_constant(LSx_train)
logm1 = sm.GLM(LSy_train, LSx_train_sm, family = sm.families.Binomial())
res = logm1.fit()
res.summary()

# Importing 'variance_inflation_factor'
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif = pd.DataFrame()
vif['Features'] = LSx_train.columns
vif['VIF'] = [variance_inflation_factor(LSx_train.values, i) for i in range(LSx_train.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif
#VIF dataframe for all the variables

"""The VIF numbers appear to be correct, but some p-values are 99%. So, take out 'What is your present occupation Housewife' and 'Last Notable Activity Had a Phone Conversation'."""

LSx_train_sm = sm.add_constant(LSx_train)
logm3 = sm.GLM(LSy_train, LSx_train_sm, family = sm.families.Binomial())
res = logm3.fit()
res.summary()

# Make a VIF dataframe for all the variables present
vif = pd.DataFrame()
vif['Features'] = LSx_train.columns
vif['VIF'] = [variance_inflation_factor(LSx_train.values, i) for i in range(LSx_train.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

LSx_train.drop('Page Views Per Visit', axis = 1, inplace = True)

# Refit the model now
X_train_sm = sm.add_constant(LSx_train)
logm3 = sm.GLM(LSy_train, LSx_train_sm, family = sm.families.Binomial())
res = logm3.fit()
res.summary()

vif = pd.DataFrame()
vif['Features'] = LSx_train.columns
vif['VIF'] = [variance_inflation_factor(LSx_train.values, i) for i in range(LSx_train.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif  #VIF dataframe for all the variables present

"""All of the VIF values are positive, and all of the p-values are less than 0.05. As a result, we can improve the algorithm.

## Prediction
"""

# Predicting the probabilities on the train set
LSy_train_pred = res.predict(LSx_train_sm)
LSy_train_pred[:10]

# Reshaping to an array
LSy_train_pred = LSy_train_pred.values.reshape(-1)
LSy_train_pred[:10]

LSy_train_pred_final = pd.DataFrame({'Converted':LSy_train.values, 'Conversion_Prob':LSy_train_pred})
LSy_train_pred_final.head()
#Data frame with specified conversion rate and prediction probability

# Substituting 0 or 1 with the cut off as 0.5
LSy_train_pred_final['Predicted'] = LSy_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)
LSy_train_pred_final.head()

"""## Model Evaluation"""

# Importing metrics from sklearn for evaluation
from sklearn import metrics

#confusion matrix 
confusion = metrics.confusion_matrix(LSy_train_pred_final.Converted, LSy_train_pred_final.Predicted )
confusion

#accuracy
metrics.accuracy_score(LSy_train_pred_final.Converted, LSy_train_pred_final.Predicted)

"""That is approximately 82% accuracy, which is a very excellent value."""

# Substituting the value of true positive
TP = confusion[1,1]
# Substituting the value of true negatives
TN = confusion[0,0]
# Substituting the value of false positives
FP = confusion[0,1] 
# Substituting the value of false negatives
FN = confusion[1,0]

# Calculating the sensitivity
TP/(TP+FN)

# Calculating the specificity
TN/(TN+FP)

"""82% accuracy, 
 sensitivity  70% and 
 specificity  88%.

## ROC Curve
"""

# ROC function
def draw_roc( actual, probs ):
    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,
                                              drop_intermediate = False )
    auc_score = metrics.roc_auc_score( actual, probs )
    plt.figure(figsize=(5, 5))
    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()

    return None

fpr, tpr, thresholds = metrics.roc_curve( LSy_train_pred_final.Converted, LSy_train_pred_final.Conversion_Prob, drop_intermediate = False )

draw_roc(LSy_train_pred_final.Converted, LSy_train_pred_final.Conversion_Prob)

"""The area under the ROC curve is 0.88, which is an excellent number."""

# Creating columns with different probability cutoffs 
numbers = [float(x)/10 for x in range(10)]
for i in numbers:
    LSy_train_pred_final[i]= LSy_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)
LSy_train_pred_final.head()

# Creating a dataframe to see the values of accuracy, sensitivity, and specificity at different values of probabiity cutoffs
cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])
# Making confusing matrix to find values of sensitivity, accurace and specificity for each level of probablity
from sklearn.metrics import confusion_matrix
num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]
for i in num:
    cm1 = metrics.confusion_matrix(LSy_train_pred_final.Converted, LSy_train_pred_final[i] )
    total1=sum(sum(cm1))
    accuracy = (cm1[0,0]+cm1[1,1])/total1
    
    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])
    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])
    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]
cutoff_df

cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])
plt.show()

"""optimal cut off is at 0.35."""

LSy_train_pred_final['final_predicted'] = LSy_train_pred_final.Conversion_Prob.map( lambda x: 1 if x > 0.35 else 0)
LSy_train_pred_final.head()

# Check the overall accuracy
metrics.accuracy_score(LSy_train_pred_final.Converted, LSy_train_pred_final.final_predicted)

#confusion matrix 
confusion2 = metrics.confusion_matrix(LSy_train_pred_final.Converted, LSy_train_pred_final.final_predicted )
confusion2

# Substituting the value of true positive
TP = confusion2[1,1]
# Substituting the value of true negatives
TN = confusion2[0,0]
# Substituting the value of false positives
FP = confusion2[0,1] 
# Substituting the value of false negatives
FN = confusion2[1,0]

#sensitivity
TP/(TP+FN)

#specificity
TN/(TN+FP)

"""With the present cut off of 0.35, we have around 80% accuracy, sensitivity, and specificity.

Conclusion
The variables that meant the most to potential buyers were discovered to be (in descending order):

TotalVisits
The total time spend on the Website.
Lead Origin_Lead Add Form
Lead Source_Direct Traffic
Lead Source_Google
Lead Source_Welingak Website
Lead Source_Organic Search
Lead Source_Referral Sites
Lead Source_Welingak Website
Do Not Email_Yes
Last Activity_Email Bounced
Last Activity_Olark Chat Conversation

Keeping these in mind, X Education can thrive because they have a very high chance of convincing almost all prospective buyers to alter their minds and purchase their courses.
"""